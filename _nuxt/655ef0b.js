(window.webpackJsonp=window.webpackJsonp||[]).push([[77],{564:function(n){n.exports=JSON.parse('{"title":"What\'s the difference between EVSIDS and LR?","subtitle":"a tentative thought","date":"2020-05-19T00:00:00.000Z","tags":["SAT"],"bodyContent":"よくわかってない人の意見ですが。\\n\\n最近のSATソルバの注目すべきアイデアの一つにLearning Rate (+ Reason-Side) Rewardingがある。\\n長らく続いてきたEVSIDS一強時代に対抗できる成果を残している。と言っても完全に上回るのではなく\\n「タイムアウトを長くした」場合という限定詞付きではあるが、それでも注目すべき技術だろう。\\nというわけで最近のSplrでもfeatureで切り替えられるようにした。\\n\\nLRのよさはなんとなくわかる。LRのアイデアは学習率の最適化により矛盾発生頻度を最大化しようとするものである[1]。\\n\\n> In particular, we model the variable selection optimization problem as an online multi-armed bandit, a special-case of reinforcement learning, to learn branching variables such that the learning rate of the solver is maximized. \\n\\n> Since producing learnt clauses is a direct indication of progress, we define our metric to be the variable’s propensity to produce learnt clauses.\\n\\nこれによってSAT問題であれば矛盾を全て解決していけば解に到達するし、UNSAT問題であればUNSATなコアに到達できるのだろう。\\n\\nで自然な考えに思えるのだが、だとすると、これとは違うEVSIDSは一体何をしていて、\\nなぜ10年以上も君臨できるほどいい結果が出せているのかが、また疑問に思えてきた。\\n\\nLRが出てくるまでは漠然と矛盾発生率の最大化だと思っていたのだけど、もはやこれとは違う説明を思いつかなければならない。\\n一体なんなんだろうかと考えていて一つの仮説を思いついた。\\nと言ってもそれほど大したものではないのだが。\\n\\nそれは、矛盾回避率の最大化ではないだろうか。そもそも矛盾が解決したはずなのにそれに関与するリテラルに対して（後付けで）報酬を与えて意味があるのかが疑問だったのだが、矛盾が解決したことによって、同じリテラル集合に対して割り当てをした場合、少なくとも先の矛盾を再び起こすことはないのだから、より割り当てを進ませることができる可能性は改善しているのではなかろうか。\\nつまり、わずかではあるがこの方向で探索を進めて矛盾しない可能性が改善している。\\n従ってこれは探索のよい枝刈り（あるいは方向付け）になっているのではなかろうか。\\n\\nこうするとこの二つを対比させることができる。\\n\\n* EVSIDSは矛盾回避率の最大化を目指す。\\n* LRは矛盾発生率の最大化を目指す。\\n\\n従ってこの二つは最大化の対象は全く逆ではないだろうか。\\n全く逆だからこそどちらでもそこそこうまく行くと。。。\\n\\nで、探索に関して少しは考えてきた人間としてはbi-directional searchなんてのを思い出すわけですよ。\\nこの両者は併存、併用できるのではないだろうか。なぜなら（ここが一番根拠の薄い所だけども）\\n\\n* EVSIDSは矛盾を避けることでSAT解に到達しようとしている。UNSAT解はその副作用。\\n* LRは矛盾のコアを見つけることでUNSAT解に到達しようとしている。SAT解はその副作用。\\n\\nということで一つの問題を全く逆の方向から解こうとしているのだからまさにbi-directional searchに見える。\\nデメリットはおそらく線形時間の速度低下。\\nbi-directional searchはうまくいかないということで決着したようだけども、今回は探索空間が本当に組合せ爆発してしているため、同じ結論にはならないように思える。\\n\\nということでSplr-0.4.0でブランチ切ってやってみた。\\n少なくとも1例しかやってないけども、そのT56ではえらい効果があった。\\nさて、本番のベンチマークではどうなるだろうか。。。ちょっと本腰を入れてコミットしてみよう。\\n\\nさらにこう考えていくと、LRにとってリスタートは必要なのだろうか？\\nEVSIDSにとってリスタートは必要なのだろうか？\\nという疑問も出てくるのだがそれはまた別の話ということで。\\n\\n## 別の考え\\n\\nあるいは、もちろんどちらも共通の尺度であって単に計算式が違うだけということもあるかもしれない。\\n\\n* EVSIDSは直近に矛盾を解消したリテラルに重きを置く\\n* LRは平均的に矛盾を導出したリテラルに重きを置く\\n* どちらも同じことだから、ウィンドウサイズを制御する変数によって両者を連続的に結合せよ。次に探索状況から制御変数を制御せよ。\\n\\n実行時間の増加につれてEVSIDSからLRに移行する試みは既にやっているのだが。。。\\n\\n## References\\n\\n[1] J. H. Liang, V. Ganesh, P. Poupart, and K. Czarnecki, “Learning Rate Based Branching Heuristic for SAT Solvers,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 9710, pp. 123–140, 2016.","bodyHtml":"<p>よくわかってない人の意見ですが。</p>\\n<p>最近のSATソルバの注目すべきアイデアの一つにLearning Rate (+ Reason-Side) Rewardingがある。\\n長らく続いてきたEVSIDS一強時代に対抗できる成果を残している。と言っても完全に上回るのではなく\\n「タイムアウトを長くした」場合という限定詞付きではあるが、それでも注目すべき技術だろう。\\nというわけで最近のSplrでもfeatureで切り替えられるようにした。</p>\\n<p>LRのよさはなんとなくわかる。LRのアイデアは学習率の最適化により矛盾発生頻度を最大化しようとするものである[1]。</p>\\n<blockquote>\\n<p>In particular, we model the variable selection optimization problem as an online multi-armed bandit, a special-case of reinforcement learning, to learn branching variables such that the learning rate of the solver is maximized.</p>\\n</blockquote>\\n<blockquote>\\n<p>Since producing learnt clauses is a direct indication of progress, we define our metric to be the variable’s propensity to produce learnt clauses.</p>\\n</blockquote>\\n<p>これによってSAT問題であれば矛盾を全て解決していけば解に到達するし、UNSAT問題であればUNSATなコアに到達できるのだろう。</p>\\n<p>で自然な考えに思えるのだが、だとすると、これとは違うEVSIDSは一体何をしていて、\\nなぜ10年以上も君臨できるほどいい結果が出せているのかが、また疑問に思えてきた。</p>\\n<p>LRが出てくるまでは漠然と矛盾発生率の最大化だと思っていたのだけど、もはやこれとは違う説明を思いつかなければならない。\\n一体なんなんだろうかと考えていて一つの仮説を思いついた。\\nと言ってもそれほど大したものではないのだが。</p>\\n<p>それは、矛盾回避率の最大化ではないだろうか。そもそも矛盾が解決したはずなのにそれに関与するリテラルに対して（後付けで）報酬を与えて意味があるのかが疑問だったのだが、矛盾が解決したことによって、同じリテラル集合に対して割り当てをした場合、少なくとも先の矛盾を再び起こすことはないのだから、より割り当てを進ませることができる可能性は改善しているのではなかろうか。\\nつまり、わずかではあるがこの方向で探索を進めて矛盾しない可能性が改善している。\\n従ってこれは探索のよい枝刈り（あるいは方向付け）になっているのではなかろうか。</p>\\n<p>こうするとこの二つを対比させることができる。</p>\\n<ul>\\n<li>EVSIDSは矛盾回避率の最大化を目指す。</li>\\n<li>LRは矛盾発生率の最大化を目指す。</li>\\n</ul>\\n<p>従ってこの二つは最大化の対象は全く逆ではないだろうか。\\n全く逆だからこそどちらでもそこそこうまく行くと。。。</p>\\n<p>で、探索に関して少しは考えてきた人間としてはbi-directional searchなんてのを思い出すわけですよ。\\nこの両者は併存、併用できるのではないだろうか。なぜなら（ここが一番根拠の薄い所だけども）</p>\\n<ul>\\n<li>EVSIDSは矛盾を避けることでSAT解に到達しようとしている。UNSAT解はその副作用。</li>\\n<li>LRは矛盾のコアを見つけることでUNSAT解に到達しようとしている。SAT解はその副作用。</li>\\n</ul>\\n<p>ということで一つの問題を全く逆の方向から解こうとしているのだからまさにbi-directional searchに見える。\\nデメリットはおそらく線形時間の速度低下。\\nbi-directional searchはうまくいかないということで決着したようだけども、今回は探索空間が本当に組合せ爆発してしているため、同じ結論にはならないように思える。</p>\\n<p>ということでSplr-0.4.0でブランチ切ってやってみた。\\n少なくとも1例しかやってないけども、そのT56ではえらい効果があった。\\nさて、本番のベンチマークではどうなるだろうか。。。ちょっと本腰を入れてコミットしてみよう。</p>\\n<p>さらにこう考えていくと、LRにとってリスタートは必要なのだろうか？\\nEVSIDSにとってリスタートは必要なのだろうか？\\nという疑問も出てくるのだがそれはまた別の話ということで。</p>\\n<h2>別の考え</h2>\\n<p>あるいは、もちろんどちらも共通の尺度であって単に計算式が違うだけということもあるかもしれない。</p>\\n<ul>\\n<li>EVSIDSは直近に矛盾を解消したリテラルに重きを置く</li>\\n<li>LRは平均的に矛盾を導出したリテラルに重きを置く</li>\\n<li>どちらも同じことだから、ウィンドウサイズを制御する変数によって両者を連続的に結合せよ。次に探索状況から制御変数を制御せよ。</li>\\n</ul>\\n<p>実行時間の増加につれてEVSIDSからLRに移行する試みは既にやっているのだが。。。</p>\\n<h2>References</h2>\\n<p>[1] J. H. Liang, V. Ganesh, P. Poupart, and K. Czarnecki, “Learning Rate Based Branching Heuristic for SAT Solvers,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 9710, pp. 123–140, 2016.</p>\\n","dir":"article/.json/2020","base":"2020-05-19-two-heuristics.json","ext":".json","sourceBase":"2020-05-19-two-heuristics.md","sourceExt":".md"}')}}]);